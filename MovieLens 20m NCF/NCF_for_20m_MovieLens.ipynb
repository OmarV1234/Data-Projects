{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "NCF for 20m MovieLens.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NCF Recommender\n",
        "\n",
        "A lot of recommender systems are build on some form of matrix factorization, but in the recent research what seems to be most popular is to do matrix factorization through some kind of neural network. This usually allows for the usual benefits of matrix factorization(scaling, complex user/item relationships, etc.) along with more line/nonlinear relationships.\n",
        "\n",
        "Neural Collaborative Filtering (NCF) is one of the most popular algorithms in this area. We'll build a simple NCF on a movielens dataset, as is standard for most Product Recommendation Algorithms."
      ],
      "metadata": {
        "id": "1YBU29tx1fcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Recommender Systems using Implicit Feedback\n",
        "\n",
        "Originally, since product recommendation only really took off after the netflix prices, a lot of product recommendation algorithms are done using \"explicit\" (meaning 1-5 stars) feedback. That said, this is pretty rare in the real world. Most feedback is in truth, implicit (1 if liked or 0 otherwise) and thats the type of data we will use in this notebook"
      ],
      "metadata": {
        "id": "ozhXXzL81fcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Before we start building and training our model, let's do some preprocessing to get the data in the required format."
      ],
      "metadata": {
        "id": "i5un-ufa1fcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "np.random.seed(123)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "MRDShkQz1fcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = pd.read_csv('movielens-20m-dataset/rating.csv', \n",
        "                      parse_dates=['timestamp'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "Kf_2lyjE1fcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this is being run on collaboratory, lets limit this to 30% of users."
      ],
      "metadata": {
        "id": "OkQnITtx1fca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rand_userIds = np.random.choice(ratings['userId'].unique(), \n",
        "                                size=int(len(ratings['userId'].unique())*0.3), \n",
        "                                replace=False)\n",
        "\n",
        "ratings = ratings.loc[ratings['userId'].isin(rand_userIds)]\n",
        "\n",
        "print('There are {} rows of data from {} users'.format(len(ratings), len(rand_userIds)))"
      ],
      "metadata": {
        "trusted": true,
        "id": "QckwYQTq1fcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.sample(5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZzsR6dx51fcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now 6,027,314  rows of data from 41,547 users"
      ],
      "metadata": {
        "id": "VyQLmmz11fce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-test split\n",
        "\n",
        "A regular train test split wouldn't really make sense here since time is particularly relevant, so we're going to split this based on the timestamp with leave out 1 interaction. That last interaction is what we need to \"predict\" to be right. \n",
        "\n",
        "There's a lot more nuance to this in the real world, but this is good enough for a start.\n",
        "\n",
        "![](https://i.imgur.com/oNJnLqU.png)\n",
        "> **Movie posters from themoviedb.org ()**\n",
        "> \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4YLnq3FY1fcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'] \\\n",
        "                                .rank(method='first', ascending=False)\n",
        "\n",
        "train_ratings = ratings[ratings['rank_latest'] != 1]\n",
        "test_ratings = ratings[ratings['rank_latest'] == 1]\n",
        "\n",
        "# drop columns that we no longer need\n",
        "train_ratings = train_ratings[['userId', 'movieId', 'rating']]\n",
        "test_ratings = test_ratings[['userId', 'movieId', 'rating']]"
      ],
      "metadata": {
        "trusted": true,
        "id": "aLTDeVrt1fci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting the dataset into an implicit feedback dataset\n",
        "\n",
        "Now we're going to turn this into an implicit feedback dataset. This does bring a few issues into the front, mainly how if its now rated its a 0 (meaning now interested) which isn't necessarily true. That said, its been shown to work in academia and industry so we can accept it for our modeling. \n",
        "\n",
        "There's also the problem of how many negative examples to include (there's always tons of negative examples). For now we're gonna include 4.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5G_1GRGQ1fck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratings.loc[:, 'rating'] = 1\n",
        "\n",
        "train_ratings.sample(5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "rqWN8f1Q1fcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a list of all movie IDs\n",
        "all_movieIds = ratings['movieId'].unique()\n",
        "\n",
        "# Placeholders that will hold the training data\n",
        "users, items, labels = [], [], []\n",
        "\n",
        "# This is the set of items that each user has interaction with\n",
        "user_item_set = set(zip(train_ratings['userId'], train_ratings['movieId']))\n",
        "\n",
        "# 4:1 ratio of negative to positive samples\n",
        "num_negatives = 4\n",
        "\n",
        "for (u, i) in tqdm(user_item_set):\n",
        "    users.append(u)\n",
        "    items.append(i)\n",
        "    labels.append(1) # items that the user has interacted with are positive\n",
        "    for _ in range(num_negatives):\n",
        "        # randomly select an item\n",
        "        negative_item = np.random.choice(all_movieIds) \n",
        "        # check that the user has not interacted with this item\n",
        "        while (u, negative_item) in user_item_set:\n",
        "            negative_item = np.random.choice(all_movieIds)\n",
        "        users.append(u)\n",
        "        items.append(negative_item)\n",
        "        labels.append(0) # items not interacted with are negative"
      ],
      "metadata": {
        "trusted": true,
        "id": "Lae7q9JI1fcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets create a PyTorch dataset class for training"
      ],
      "metadata": {
        "id": "5M0pProU1fcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MovieLensTrainDataset(Dataset):\n",
        "    \"\"\"MovieLens PyTorch Dataset for Training\n",
        "    \n",
        "    Args:\n",
        "        ratings (pd.DataFrame): Dataframe containing the movie ratings\n",
        "        all_movieIds (list): List containing all movieIds\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ratings, all_movieIds):\n",
        "        self.users, self.items, self.labels = self.get_dataset(ratings, all_movieIds)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "  \n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.items[idx], self.labels[idx]\n",
        "\n",
        "    def get_dataset(self, ratings, all_movieIds):\n",
        "        users, items, labels = [], [], []\n",
        "        user_item_set = set(zip(ratings['userId'], ratings['movieId']))\n",
        "\n",
        "        num_negatives = 4\n",
        "        for u, i in user_item_set:\n",
        "            users.append(u)\n",
        "            items.append(i)\n",
        "            labels.append(1)\n",
        "            for _ in range(num_negatives):\n",
        "                negative_item = np.random.choice(all_movieIds)\n",
        "                while (u, negative_item) in user_item_set:\n",
        "                    negative_item = np.random.choice(all_movieIds)\n",
        "                users.append(u)\n",
        "                items.append(negative_item)\n",
        "                labels.append(0)\n",
        "\n",
        "        return torch.tensor(users), torch.tensor(items), torch.tensor(labels)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ftZLKK8m1fco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Our model - Neural Collaborative Filtering (NCF)\n",
        "\n",
        "As mentioned before, we're going to use NCF to model this dataset[He et al.](https://arxiv.org/abs/1708.05031) \n",
        "\n",
        "<img src=\"https://recodatasets.z20.web.core.windows.net/images/NCF.svg?sanitize=true\">\n",
        "\n",
        "Its a pretty popular model building on past research on matrix factorization and neural networks.fundamentally, we're building 2 embedding layers for users and items which we put through a \"shallow layer\" and a \"deep layer\" in order to make our predictions.\n"
      ],
      "metadata": {
        "id": "c2V9TV311fcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NCF(pl.LightningModule):\n",
        "    \"\"\" Neural Collaborative Filtering (NCF)\n",
        "    \n",
        "        Args:\n",
        "            num_users (int): Number of unique users\n",
        "            num_items (int): Number of unique items\n",
        "            ratings (pd.DataFrame): Dataframe containing the movie ratings for training\n",
        "            all_movieIds (list): List containing all movieIds (train + test)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_users, num_items, ratings, all_movieIds):\n",
        "        super().__init__()\n",
        "        self.user_embedding = nn.Embedding(num_embeddings=num_users, embedding_dim=8)\n",
        "        self.item_embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=8)\n",
        "        self.fc1 = nn.Linear(in_features=16, out_features=64)\n",
        "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
        "        self.output = nn.Linear(in_features=32, out_features=1)\n",
        "        self.ratings = ratings\n",
        "        self.all_movieIds = all_movieIds\n",
        "        \n",
        "    def forward(self, user_input, item_input):\n",
        "        \n",
        "        # Pass through embedding layers\n",
        "        user_embedded = self.user_embedding(user_input)\n",
        "        item_embedded = self.item_embedding(item_input)\n",
        "\n",
        "        # Concat the two embedding layers\n",
        "        vector = torch.cat([user_embedded, item_embedded], dim=-1)\n",
        "\n",
        "        # Pass through dense layer\n",
        "        vector = nn.ReLU()(self.fc1(vector))\n",
        "        vector = nn.ReLU()(self.fc2(vector))\n",
        "\n",
        "        # Output layer\n",
        "        pred = nn.Sigmoid()(self.output(vector))\n",
        "\n",
        "        return pred\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        user_input, item_input, labels = batch\n",
        "        predicted_labels = self(user_input, item_input)\n",
        "        loss = nn.BCELoss()(predicted_labels, labels.view(-1, 1).float())\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters())\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(MovieLensTrainDataset(self.ratings, self.all_movieIds),\n",
        "                          batch_size=512, num_workers=4)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Y0g7Xsi21fcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_users = ratings['userId'].max()+1\n",
        "num_items = ratings['movieId'].max()+1\n",
        "\n",
        "all_movieIds = ratings['movieId'].unique()\n",
        "\n",
        "model = NCF(num_users, num_items, train_ratings, all_movieIds)"
      ],
      "metadata": {
        "trusted": true,
        "id": "a_9MCgeo1fcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train our NCF model for 5 epochs using the GPU."
      ],
      "metadata": {
        "id": "0Ne1e2U31fcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = pl.Trainer(max_epochs=5, gpus=1, reload_dataloaders_every_epoch=True,\n",
        "                     progress_bar_refresh_rate=50, logger=False, checkpoint_callback=False)\n",
        "\n",
        "trainer.fit(model)"
      ],
      "metadata": {
        "trusted": true,
        "id": "2r5jLUP81fct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating our Recommender System\n",
        "\n",
        "Now lets evaluate the model. Generally, product recommendations are used in the following way in the real world:\n",
        "\n",
        "![](https://i.imgur.com/XZZ2Ni8.png)\n",
        "\n",
        "You get a list of recommendations (not just one) and if you click/buy an item on the list, its a \"hit\". This is what we would want to emulate in our metrics.\n",
        "\n",
        "To emulate this, Lets do the following:\n",
        "\n",
        "* For each user, randomly select 99 items that the user **has not interacted with**\n",
        "* Combine these 99 items with the test item (the actual item that the user interacted with). We now have 100 items.\n",
        "* Run the model on these 100 items, and rank them according to their predicted probabilities\n",
        "* Select the top 10 items from the list of 100 items. If the test item is present within the top 10 items, then we say that this is a hit.\n",
        "* Repeat the process for all users. The Hit Ratio is then the average hits.\n",
        "\n",
        "This model metric is usually called **Hit Ratio @ 10**."
      ],
      "metadata": {
        "id": "ScriYYsp1fct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hit Ratio @ 10 \n",
        "\n"
      ],
      "metadata": {
        "id": "tIDcbQWk1fcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User-item pairs for testing\n",
        "test_user_item_set = set(zip(test_ratings['userId'], test_ratings['movieId']))\n",
        "\n",
        "# Dict of all items that are interacted with by each user\n",
        "user_interacted_items = ratings.groupby('userId')['movieId'].apply(list).to_dict()\n",
        "\n",
        "hits = []\n",
        "for (u,i) in tqdm(test_user_item_set):\n",
        "    interacted_items = user_interacted_items[u]\n",
        "    not_interacted_items = set(all_movieIds) - set(interacted_items)\n",
        "    selected_not_interacted = list(np.random.choice(list(not_interacted_items), 99))\n",
        "    test_items = selected_not_interacted + [i]\n",
        "    \n",
        "    predicted_labels = np.squeeze(model(torch.tensor([u]*100), \n",
        "                                        torch.tensor(test_items)).detach().numpy())\n",
        "    \n",
        "    top10_items = [test_items[i] for i in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
        "    \n",
        "    if i in top10_items:\n",
        "        hits.append(1)\n",
        "    else:\n",
        "        hits.append(0)\n",
        "        \n",
        "print(\"The Hit Ratio @ 10 is {:.2f}\".format(np.average(hits)))"
      ],
      "metadata": {
        "trusted": true,
        "id": "v44o72d_1fcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To put this into context, what this means is that 86% of the users were recommended the actual item (among a list of 10 items) that they eventually interacted with. This is honestly better than is likely in the real world, but not bad for practice models"
      ],
      "metadata": {
        "id": "mT3-iMlI1fcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KzOtC_E81fcx"
      }
    }
  ]
}